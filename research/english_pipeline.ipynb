{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78b43a8d",
   "metadata": {},
   "source": [
    "Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "929fa8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Download required NLTK assets\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "# Configuration\n",
    "API_URL = \"http://localhost:8000/api/retrieve/cmtsep/en\"\n",
    "DB_CONFIG = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"database\": \"data_pipeline\",\n",
    "    \"user\": \"djangouser\",\n",
    "    \"password\": \"django123\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf29b0ed",
   "metadata": {},
   "source": [
    "Fetching Data from Django API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fd744c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Contacting Django API...\n",
      "[+] Found 5 new comments to process.\n"
     ]
    }
   ],
   "source": [
    "def fetch_english_comments():\n",
    "    print(\"[*] Contacting Django API...\")\n",
    "    try:\n",
    "        response = requests.get(API_URL)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        if df.empty:\n",
    "            print(\"[!] No English comments found.\")\n",
    "            return None\n",
    "            \n",
    "        # We only want to process rows where cleaned_text is currently empty/None\n",
    "        # This handles the \"prior pre-processed data\" requirement\n",
    "        if 'cleaned_text' in df.columns:\n",
    "            unprocessed = df[df['cleaned_text'].isna() | (df['cleaned_text'] == \"\")]\n",
    "        else:\n",
    "            unprocessed = df\n",
    "            \n",
    "        print(f\"[+] Found {len(unprocessed)} new comments to process.\")\n",
    "        return unprocessed\n",
    "    except Exception as e:\n",
    "        print(f\"[X] API Error: {e}\")\n",
    "        return None\n",
    "\n",
    "df_raw = fetch_english_comments()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac0dd86",
   "metadata": {},
   "source": [
    "Cleaning & Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f11ca88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Running Cleaning Pipeline...\n",
      "[+] Cleaning complete. 5 rows ready.\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    if not text: return \"\"\n",
    "    \n",
    "    # 1. Remove URLs, HTML tags, and non-alphabetic characters\n",
    "    text = re.sub(r'http\\S+|www\\S+|<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # 2. Tokenize and Lowercase\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    # 3. Remove Stopwords and Lemmatize\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Keep words longer than 2 characters\n",
    "    cleaned = [lemmatizer.lemmatize(w) for w in words if w not in stop_words and len(w) > 2]\n",
    "    \n",
    "    return \" \".join(cleaned)\n",
    "\n",
    "if df_raw is not None:\n",
    "    print(\"[*] Running Cleaning Pipeline...\")\n",
    "    df_raw['cleaned_text'] = df_raw['comment'].apply(clean_text)\n",
    "    # Drop rows that result in empty strings after cleaning\n",
    "    df_clean = df_raw[df_raw['cleaned_text'] != \"\"].copy()\n",
    "    print(f\"[+] Cleaning complete. {len(df_clean)} rows ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d14f58",
   "metadata": {},
   "source": [
    "Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f033eb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Generating TF-IDF Vectors...\n",
      "[+] Vectorization successful. Vocabulary size: 2\n"
     ]
    }
   ],
   "source": [
    "if df_raw is not None:\n",
    "    print(\"[*] Generating TF-IDF Vectors...\")\n",
    "    vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(df_clean['cleaned_text'])\n",
    "    \n",
    "    # Capture feature names (words) for later topic modeling\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    print(f\"[+] Vectorization successful. Vocabulary size: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f45d16e",
   "metadata": {},
   "source": [
    "Save Results Back to Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f303ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Connecting to database to save results...\n",
      "[!] Successfully updated 5 rows in 'processed_comments'.\n",
      "[*] Database connection closed.\n"
     ]
    }
   ],
   "source": [
    "# STEP 5: SAVE TO DATABASE\n",
    "def update_database(df):\n",
    "    if df.empty: \n",
    "        print(\"[!] No data to save.\")\n",
    "        return\n",
    "    \n",
    "    conn = None\n",
    "    cur = None\n",
    "    \n",
    "    print(\"[*] Connecting to database to save results...\")\n",
    "    try:\n",
    "        # Using 127.0.0.1 to avoid Windows localhost issues\n",
    "        conn = psycopg2.connect(\n",
    "            host=\"127.0.0.1\", \n",
    "            database=\"data_pipeline\", \n",
    "            user=\"admin\", \n",
    "            password=\"admin\" \n",
    "        )\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # Prepare data for the update\n",
    "        data_to_update = list(df[['id', 'cleaned_text']].itertuples(index=False, name=None))\n",
    "        \n",
    "        # This query updates the existing rows created by Airflow\n",
    "        update_query = \"\"\"\n",
    "            UPDATE airflow.processed_comments \n",
    "            SET cleaned_text = data.new_cleaned_text,\n",
    "                updated_at = CURRENT_TIMESTAMP\n",
    "            FROM (VALUES %s) AS data (cid, new_cleaned_text)\n",
    "            WHERE airflow.processed_comments.comment_id = data.cid;\n",
    "        \"\"\"\n",
    "        \n",
    "        execute_values(cur, update_query, data_to_update)\n",
    "        \n",
    "        conn.commit()\n",
    "        print(f\"[!] Successfully updated {len(data_to_update)} rows in 'processed_comments'.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[X] Database Error: {e}\")\n",
    "        if conn:\n",
    "            conn.rollback()\n",
    "    finally:\n",
    "        if cur is not None:\n",
    "            cur.close()\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "        print(\"[*] Database connection closed.\")\n",
    "\n",
    "# Finally, call the function\n",
    "if 'df_clean' in locals() and df_clean is not None:\n",
    "    update_database(df_clean)\n",
    "else:\n",
    "    print(\"[X] df_clean not found. Make sure you ran the cleaning cell first!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
